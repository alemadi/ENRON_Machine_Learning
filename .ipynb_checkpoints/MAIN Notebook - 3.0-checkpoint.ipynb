{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.93      0.95        40\n",
      "        1.0       0.50      0.75      0.60         4\n",
      "\n",
      "avg / total       0.93      0.91      0.92        44\n",
      "\n",
      "AUC - ROC :  0.8375\n",
      "Searching...\n",
      "Best parameters: \n",
      "{'LR__intercept_scaling': 100, 'LR__penalty': 'l1', 'LR__C': 0.2976351441631313}\n",
      "GridSearch classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.93      0.95        40\n",
      "        1.0       0.50      0.75      0.60         4\n",
      "\n",
      "avg / total       0.93      0.91      0.92        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from Missing_Values import remove_nulls\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi', 'poi_contact_ratio1', 'poi_contact_ratio2','salary', \n",
    "                 'to_messages', 'total_payments', 'bonus', \n",
    "                 'total_stock_value', 'shared_receipt_with_poi',\n",
    "                 'exercised_stock_options', 'from_messages',\n",
    "                 'other', 'from_this_person_to_poi', 'expenses', \n",
    "                 'restricted_stock', 'from_poi_to_this_person', \n",
    "                'total_worth', 'exp/net' ] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL',0)\n",
    "data_dict = remove_nulls(data_dict, .5)\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "df = df.replace('NaN', np.nan)\n",
    "\n",
    " # net worth feature\n",
    "net_worth =['salary', 'total_payments', 'bonus', 'total_stock_value',\n",
    "            'exercised_stock_options', 'restricted_stock']\n",
    "df['total_worth'] = df[net_worth].sum(1)\n",
    "\n",
    " # expenses:networth ratio feature\n",
    "df['exp/net'] = df.expenses / df.total_worth\n",
    "\n",
    " # from person to poi ratio feature\n",
    "df['poi_contact_ratio1']  = pd.to_numeric(df['from_this_person_to_poi']) / pd.to_numeric(df['to_messages'])\n",
    "\n",
    " # from poi to person ratio feature\n",
    "df['poi_contact_ratio2'] = df.from_poi_to_this_person / df.from_messages\n",
    "\n",
    "# re-transform to dict \n",
    "data_dict = df.to_dict(orient='index')\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "\n",
    "steps = [\n",
    "    ('PCA', PCA(random_state=42)),\n",
    "    ('LR', LogisticRegression(random_state=42,\n",
    "                              penalty = 'l1', C=0.3,\n",
    "                              intercept_scaling = 100))]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# Metrics\n",
    "print 'Pipeline classification report'\n",
    "print classification_report(y_test, y_pred)\n",
    "\n",
    "print \"AUC - ROC : \", roc_auc_score(y_test, y_pred)\n",
    "\n",
    "clf = pipeline\n",
    "\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "\n",
    "print 'Searching...'\n",
    "params = {'LR__C': np.logspace(-10, 10, 20),\n",
    "         'LR__penalty': ['l1', 'l2'],\n",
    "         'LR__intercept_scaling': [1, 10 , 100, 1000,10000,1000000]}\n",
    "\n",
    "grid = GridSearchCV(pipeline, params,cv=5, scoring='f1')\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid_pred = grid.predict(X_test)\n",
    "print 'Best parameters: '\n",
    "print grid.best_params_\n",
    "print 'GridSearch classification report:'\n",
    "print classification_report(y_test, grid_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
